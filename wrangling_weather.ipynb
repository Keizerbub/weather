{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keizerbub/weather/blob/main/wrangling_weather.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7hN_a1sAJDc"
      },
      "source": [
        "#Import framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHZ7reBiAQG5",
        "outputId": "ce1d6214-fff8-42bf-8e7e-c7c81c20d73f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=bcbc00fcd6f80cbae7f08cae6e42972827419b8fa239654b846db187d75e9102\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iRVYxUDA_mV1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDbwIX99Amm9"
      },
      "source": [
        "#ouverture des fichiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5-tNrKcwA3Bc"
      },
      "outputs": [],
      "source": [
        "\"\"\"ouverture des fichiers\"\"\"\n",
        "class wrangling:\n",
        "    def __init__(self):\n",
        "        self.spark = SparkSession.builder \\\n",
        "            .appName(\"HandlingFile\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "\n",
        "\n",
        "    def open_file(self, file_path):\n",
        "        try:\n",
        "            # Read the file into a DataFrame\n",
        "            self.df = self.spark.read.csv(file_path, header=True, inferSchema=True, sep=\";\")  # Example for reading a CSV file\n",
        "            # If it's a different type of file, use the appropriate method like `spark.read.json()`, etc.\n",
        "\n",
        "            # Show the DataFrame content\n",
        "            self.df.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Error occurred:\", e)\n",
        "\n",
        "\n",
        "\n",
        "    def close_spark(self):\n",
        "        # Stop the SparkSession\n",
        "        self.spark.stop()\n",
        "\n",
        "\n",
        "\n",
        "    def select_columns(self, *columns, columns_state=True):\n",
        "        if columns_state==False:\n",
        "          columns='LAT','LON','AAAAMMJJHH','RR1','FF','TN50'\n",
        "\n",
        "        try:\n",
        "            # Select only the desired columns\n",
        "            self.df = self.df.select(*columns)\n",
        "\n",
        "            # Show the DataFrame content after selecting columns\n",
        "            print(\"DataFrame after selecting columns:\")\n",
        "            self.df.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Error occurred:\", e)\n",
        "\n",
        "\n",
        "\n",
        "    def check_column_null(self, column_name):\n",
        "        try:\n",
        "            # Check if the column has any null values\n",
        "            null_count = self.df.filter(self.df[column_name].isNull()).count()\n",
        "\n",
        "            # Calculate the percentage of null values\n",
        "            total_rows = self.df.count()\n",
        "            self.null_percentage = (null_count / total_rows) * 100\n",
        "\n",
        "\n",
        "            if null_count > 0:\n",
        "                print(f\"Column '{column_name}' has {null_count} null values for {self.null_percentage}.\\n\")\n",
        "            else:\n",
        "                print(f\"Column '{column_name}' has no null values.\\n\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Error occurred:\", e)\n",
        "\n",
        "\n",
        "\n",
        "    def check_all_columns_null(self):\n",
        "      try:\n",
        "          # Get all column names\n",
        "          columns = self.df.columns\n",
        "\n",
        "          # Apply check_column_null to each column\n",
        "          for col in columns:\n",
        "              self.check_column_null(col)\n",
        "              #removing the missing col\n",
        "              if self.null_percentage==100.0:\n",
        "                  self.df = self.df.drop(col)\n",
        "                  print(f'the column {col} was delete')\n",
        "\n",
        "      except Exception as e:\n",
        "          print(\"Error occurred:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYcjCJChuM21",
        "outputId": "d195f3df-6efd-449f-bf95-e7676143de2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'LAT' has no null values.\n",
            "\n",
            "Column 'LON' has no null values.\n",
            "\n",
            "Column 'AAAAMMJJHH' has no null values.\n",
            "\n",
            "Column 'RR1' has 84239 null values for 91.67374034171291.\n",
            "\n",
            "Column 'FF' has 22060 null values for 24.006964849276308.\n",
            "\n",
            "Column 'TN50' has 90170 null values for 98.12819675699205.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "folder=\"\"\n",
        "data=wrangling()\n",
        "final_departement_folder = os.path.join(folder, 'final_departement')\n",
        "\n",
        "# Liste des fichiers zip dans le dossier\n",
        "files = [f for f in os.listdir(final_departement_folder) if f.endswith('.csv')]\n",
        "\n",
        "\n",
        "\n",
        "# Dictionnaire pour stocker les DataFrames par préfixe de nom de fichier\n",
        "file_dict = {}\n",
        "\n",
        "# Parcourir chaque fichier zip\n",
        "for fichier in files:\n",
        "    # Chemin complet du fichier zip\n",
        "    file_path_ = os.path.join(final_departement_folder, fichier)\n",
        "\n",
        "    # Extraire le contenu du fichier zip dans le dossier de sortie\n",
        "    data.open_file(file_path=file_path_)\n",
        "    data.select_columns(columns_state=False)\n",
        "    data.check_all_columns_null()\n",
        "    # résultat\n",
        "    print(f'file: {files} effectué\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YI83-qPcROMSM0vQvoYfB29JLrxsdUny",
      "authorship_tag": "ABX9TyPeJLYVNi5K/vcGfduPbw8J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}